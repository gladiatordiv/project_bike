






# create the .kaggle directory and an empty kaggle.json file
get_ipython().getoutput("mkdir -p /root/.kaggle")
get_ipython().getoutput("touch /root/.kaggle/kaggle.json")
get_ipython().getoutput("chmod 600 /root/.kaggle/kaggle.json")



import os
os.environ['KAGGLE_USERNAME'] = "divyachauhan3301" 
os.environ['KAGGLE_KEY'] = "b432b73086e4ceea25feacbe5ce61239"
get_ipython().getoutput("kaggle competitions download -c bike-sharing-demand")


get_ipython().getoutput("unzip -o bike-sharing-demand.zip")









import pandas as pd
from autogluon.tabular import TabularPredictor





train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')
submission = pd.read_csv('sampleSubmission.csv')


train.head()


test.head()


submission.head()


predictor = TabularPredictor(
    label="count", problem_type="regression", eval_metric="rmse"
    ).fit(
    train_data=train.drop(['casual', 'registered'], axis=1),
    time_limit=600,
    presets='best_quality')


predictor.fit_summary()


predictions = predictor.predict(test)
predictions = {'datetime': test['datetime'], 'Pred_count': predictions}
predictions = pd.DataFrame(data=predictions)
predictions.head()





predictions.describe()


negative = predictions.groupby(predictions['Pred_count'])

# lambda function
def minus(val):
   return val[val < 0].sum()

print(negative['Pred_count'].agg([('negcount', minus)]))


predictions[predictions['Pred_count']<0] = 0


predictions.describe()


predictions.head()


submission["count"] = predictions['Pred_count']
submission.to_csv("submission.csv", index=False)


import kaggle



get_ipython().getoutput("kaggle competitions submit -c bike-sharing-demand -f submission.csv -m "first raw submission"")


get_ipython().getoutput("kaggle competitions submissions -c bike-sharing-demand | tail -n +1 | head -n 6")


train.hist()


train['datetime'] = pd.to_datetime(train['datetime'])
test['datetime'] = pd.to_datetime(test['datetime'])

# Access year, month, and day
train['year'] = train['datetime'].dt.year
train['month'] = train['datetime'].dt.month
train['day'] = train['datetime'].dt.day
train['hour'] = train['datetime'].dt.hour

test['year'] = test['datetime'].dt.year
test['month'] = test['datetime'].dt.month
test['day'] = test['datetime'].dt.day
test['hour'] = test['datetime'].dt.hour


train["season"] = train["season"].astype("category")
train["weather"] = train["weather"].astype("category")
test["season"] = test["season"].astype("category")
test["weather"] = test["weather"].astype("category")


train.head()


train.hist()


predictor_new_features = TabularPredictor(
    label="count", problem_type="regression", eval_metric="rmse"
    ).fit(
    train_data=train.drop(['casual', 'registered'], axis=1),
    time_limit=600,
    presets='best_quality')


predictor_new_features.fit_summary()


predictions_new_features = predictor_new_features.predict(test)
predictions_new_features = {'datetime': test['datetime'], 'Pred_count': predictions_new_features}
predictions_new_features = pd.DataFrame(data=predictions_new_features)
predictions_new_features.head()


predictions_new_features[predictions_new_features['Pred_count']<0] = 0


predictions_new_features.describe()


# Same submitting predictions
submission_new_features = pd.read_csv('submission.csv')
submission_new_features["count"] = predictions_new_features['Pred_count']
submission_new_features.to_csv("submission_new_features.csv", index=False)


get_ipython().getoutput("kaggle competitions submit -c bike-sharing-demand -f submission_new_features.csv -m "new features"")


get_ipython().getoutput("kaggle competitions submissions -c bike-sharing-demand | tail -n +1 | head -n 6")





import autogluon.core as ag

nn_options = {  # specifies non-default hyperparameter values for neural network models
    'num_epochs': 10,  # number of training epochs (controls training time of NN models)
    'learning_rate': ag.space.Real(1e-4, 1e-2, default=5e-4, log=True),  # learning rate used in training (real-valued hyperparameter searched on log-scale)
    'activation': ag.space.Categorical('relu', 'softrelu', 'tanh'),  # activation function used in NN (categorical hyperparameter, default = first entry)
    'layers': ag.space.Categorical([100], [1000], [200, 100], [300, 200, 100]),  # each choice for categorical hyperparameter 'layers' corresponds to list of sizes for each NN layer to use
    'dropout_prob': ag.space.Real(0.0, 0.5, default=0.1),  # dropout probability (real-valued hyperparameter)
}

gbm_options = {  # specifies non-default hyperparameter values for lightGBM gradient boosted trees
    'num_boost_round': 100,  # number of boosting rounds (controls training time of GBM models)
    'num_leaves': ag.space.Int(lower=26, upper=66, default=36),  # number of leaves in trees (integer hyperparameter)
}

hyperparameters = {  # hyperparameters of each model type
                   'GBM': gbm_options,
                   #'NN': nn_options,  # NOTE: comment this line out if you get errors on Mac OSX
                  }  # When these keys are missing from hyperparameters dict, no models of that type are trained

#num_trials = 5  # try at most 5 different hyperparameter configurations for each type of model
search_strategy = 'auto'  # to tune hyperparameters using Bayesian optimization routine with a local scheduler
hyperparameter_tune_kwargs = {  # HPO is not performed unless hyperparameter_tune_kwargs is specified
    #'num_trials': num_trials,
    'scheduler' : 'local',
    'searcher': search_strategy,
}

predictor_new_hpo = TabularPredictor(label="count", eval_metric="root_mean_squared_error",learner_kwargs={"ignored_columns":
["casual", "registered"]}).fit(train_data=train, time_limit=600, presets="best_quality", hyperparameters=hyperparameters, hyperparameter_tune_kwargs=hyperparameter_tune_kwargs,
)


predictor_new_hpo.fit_summary()


new_predictions_hpo = predictor_new_hpo.predict(test)
new_predictions_hpo[new_predictions_hpo<0] = 0


# Same submitting predictions
submission_new_hpo = pd.read_csv("submission.csv", parse_dates=["datetime"])
submission_new_hpo["count"] = new_predictions_hpo
submission_new_hpo.to_csv("submission_new_hpo.csv", index=False)


get_ipython().getoutput("kaggle competitions submit -c bike-sharing-demand -f submission_new_hpo.csv -m "new features with hyperparameters"")


get_ipython().getoutput("kaggle competitions submissions -c bike-sharing-demand | tail -n +1 | head -n 6")


fig = pd.DataFrame(
    {
        "model": ["initial", "add_features", "hpo"],
        "score": [-34.314234, -35.149822, -36.227587]
    }
).plot(x="model", y="score", figsize=(10, 8)).get_figure()
fig.savefig('model_train_score.png')


# Take the 3 kaggle scores and creating a line plot to show improvement
fig = pd.DataFrame(
    {
        "test_eval": ["initial", "add_features", "hpo"],
        "score": [1.80512, 0.65798, 0.49696]
    }
).plot(x="test_eval", y="score", figsize=(10, 8)).get_figure()
fig.savefig('model_test_score.png')


pd.DataFrame({
    "model": ["initial", "add_features", "hpo"],
    "timelimit": ["time_limit = 600", "time_limit=600", "time_limit=600"],
    "presets": ["presets='best_quality'", "presets='best_quality'", "presets='best_quality'"],
    "hp-method": ["none", "problem_type = 'regression'", "tabular autogluon"],
    "score": ['1.80512',' 0.65798', '0.49696']
})


def plot_series(time, series, format="-", start=0, end=None, label=None):
    plt.plot(time[start:end], series[start:end], format, label=label)
    plt.xlabel("Time")
    plt.ylabel("Value")
    if label:
        plt.legend(fontsize=14)
    plt.grid(True)


sub_new = pd.read_csv('submission_new_features.csv')


import matplotlib.pyplot as plt
series = train["count"].to_numpy()
time = train["datetime"].to_numpy()


plt.figure(figsize=(350, 50))
plot_series(time, series)
plt.title("Train Data time series graph")
#plot_series(time1, series1)
plt.show()


sub_new.loc[:, "datetime"] = pd.to_datetime(sub_new.loc[:, "datetime"])

series1 = sub_new["count"].to_numpy()
time1 = sub_new["datetime"].to_numpy()

plt.figure(figsize=(400, 50))
#plot_series(time, series)
plot_series(time1, series1)
plt.title("Test Data time series graph")
plt.show()


import xgboost as xgb


train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')



train_df['datetime'] = pd.to_datetime(train['datetime'])
test_df['datetime'] = pd.to_datetime(test['datetime'])

train_df['year'] = train_df['datetime'].dt.year
train_df['month'] = train_df['datetime'].dt.month
train_df['day'] = train_df['datetime'].dt.day
train_df['hour'] = train_df['datetime'].dt.hour

test_df['year'] = test_df['datetime'].dt.year
test_df['month'] = test_df['datetime'].dt.month
test_df['day'] = test_df['datetime'].dt.day
test_df['hour'] = test_df['datetime'].dt.hour


trainxgb = train_df.drop(['casual', 'registered','count', 'datetime'], axis=1)
trainxgb.head()


countxgb = train_df['count']
countxgb.head()


train_xgb = xgb.DMatrix(
    trainxgb, countxgb
)

params = {"objective": "reg:linear"} 
bst = xgb.train(params, train_xgb)

bst.predict(train_xgb)


get_ipython().getoutput("jupyter nbconvert --to html project_notebook.ipynb")






